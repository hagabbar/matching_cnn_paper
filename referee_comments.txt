Dear XXXXXX,

Thank you for sending us the referees reports, who we thank for
reading our manuscript and for providing very useful input. We are delighted by
the overall positivity of the referees and have addressed their
comments in the text below.

Best wishes,

H. Gabbard, M. Williams, F. Hayes, and C. Messenger

##################
Referee A Comments
##################

1. It is claimed in a number of places in the text that the
computational cost of searches for compact object mergers is tied to
improvements in the low-frequency sensitivity of gravitational-wave
detectors. For example, in the abstract there is the statement

"However, the computational cost of such searches in low latency will
grow dramatically as the low frequency sensitivity of
gravitational-wave detectors improves."

and in the second paragraph of the Introduction

"The computational cost to run the search analysis is due to the large
parameter space and the increasing cost of analyzing longer duration
waveforms as the low frequency sensitivity of the detectors improves."

These claims are not true. They would be true for a naive,
brute-force, template bank + matched-filter approach, as was used in
the early days of LIGO, but they are not true for modern detection
systems. The low frequency parts of the signals have two properties
relevant here: firstly, they're low frequency meaning the filtering of
these parts of the waveforms can be computed at a lower sample rate;
secondly the rate of frequency evolution at low frequencies is small
due to the slow rate of energy loss from the system, making the
structure of the waveforms more and more simple at low frequencies.
Modern detection systems, for example the Multi-Band Template Analysis
(MBTA) and GstLAL-based detection systems, take advantage of both of
these properties to remove redundancy from the calculations and reduce
the computational cost. The result is that the cost is nearly
independent of the starting frequency; the computational cost is
entirely dominated by the high frequency parts of the waveform where
high data sample rates are required and complex frequency and
amplitude evolution are observed.

I cannot find anywhere in this document an accounting of the
computational cost of the convolutional neural network nor of the
matched filtering approach, and I feel that without that information a
discussion of the computational cost of either of the approaches, or
in particular their scalings, is out of place. I will raise this issue
again below, but I think perhaps removing any sort of claims of speed,
cost, whatever, is warranted, unless a comparative study is presented
to support them.

# 1)* --- (Hunter) If we decide to keep in the bit where we describe the source
# of computational cost in a matched filtering analysis, then we should also
# say that the majority of the expense comes from the high frequency portion of
# the waveform. What are folks thoughts about keeping these statements in? I
# was of the opinion that it is important to keep them, given that the
# advantage of using a neural network is that it can run faster than a matched
# template filtering analysis, however it would entail quite a bit of work to
# make the appropriate comparisons. Referee A's response in comment 5 is what
# changed my mind. 

# (Hunter) I've gone ahead and commented out the references to computational cost.

(team response) We believe that the referee is entirely correct in asking for
these claims to be removed from the paper. We have therefore removed statements
that make explicit claims of speed improvements over current advanced matched
filtering approaches. We have now included a statement in the manuscript text
that gives the actual time taken to both train and run the CNN analysis but we
do not compare these values with speeds or scaling relations for existing
matched-filter approaches (see our response to comment 5 for the additional
text). As the referee points out in further comments below, the main result of
the paper alone is worthy of publication in PRL. We will leave a detailed study
of comparative computational cost to future work.  

2. Introduction, second paragraph, "... in these search pipelines is
achieved using a technique known as ..." --> ... in these search
pipelines is achieved, in part, using a technique known as ..." In
fact the process also makes of use various \chi^2-like statistics, and
various measures of signal consistency across the multiple detectors
world-wide.

# 2)* --- (Hunter) Fixed.

(team response) This sentence has been changed accordingly.

3. Introduction, third paragraph, "... each with different component
mass components and/or ..." --> "... each with different component
mass and/or ..."

# 3.)* --- (Hunter) Fixed.

(team response) This typo has been fixed.


4. Introduction, fourth paragraph, the claim "This results in
low-latency searches that can be orders of magnitude faster than other
comparable classification methods." requires a reference. There are
weasel words in there, "can be", "comparable" (what are "comparable"
methods? only the ones for which this statement is true?), and because
of that strictly speaking no specific claim is being made here, but a
claim is being implied and I am certain that claim is false. I am
reminded of similar claims made about GPU-based FFT algorithms: orders
of magnitude faster, 100x faster, and so on. It is true that the GPU
can perform a subset of the tasks required to identify
gravitational-wave signals at an enormously higher speed than a CPU,
but it can't do all of the tasks at that speed. If 50% of the tasks
can be done so quickly that they are reduced to 0 time, you have only
increased the speed by a factor of 2, not 100s or 1000s, because there
remains the other 50% of the tasks that still take just as long.
Meanwhile, if you merely wait 18 months Moore's Law will give you that
factor of 2, anyway, without having to spend time porting your
algorithm to GPUs. This is the Achilles' heal of GPUs: they're a lot
of work, and there are things they're not good at, which is why the
use of GPUs for matched filtering based GW detection systems has never
gained traction. Anyway, if a reference to support this claim cannot
be found it should be removed.

# 4)* --- (Hunter) I removed the above statement from the manuscript.
# (Hunter) Need to come up with a more concrete respose.

# (team response) A large advantage of using a machine learning algorithm
# to do CBC searches resides in the low-latency nature of doing such a
# search. It is true that not all tasks of a search analysis can be 
# tabulated on a GPU, but that is besides the point. For the purpose
# of simply classifying CBC signals, the vast majority of computational 
# expense is spent in the training process, whereas running after training
# on new data is done in a matter of seconds regardless of the size of the
# testing set. Retraining is not necessarily required for every new
# search analysis run, so network runtime would be inconsequential. 

4)* --- (Chris) This point is related to the first comment and we are happy to
remove statements that refer to definitive speed improvements over traditional
methods. We do this for the clear scientific reason given by the referee, that
we have no concrete evidence of this at the present time.

However, we would like to clarify some key points to the referee in light of
their statement that they are certain our speed claims are false. This sounds
equally like a statement without evidence to support it. Historical claims
regarding the speed improvement using GPUs are not relevant here. Yes, GPUs are
key to the sucess of neural network applications in recent years but the more
important (potential) speed gains are due to the algorithm itself together with
the division of computational effort into training and testing. It is possible that
the "total" computational cost of CNNs exceeds that of traditional techniques,
but only in the training stage prior to the analysis. The testing stage is
incredibly fast but as highlighted by the referee, we do not have any scaling
relations to refer to for CNNs at present. For the referees information, the
analysis of 25K 1sec segments of data runs on a single GPU in ~few sec.

5. I am also puzzled by the claim that has been made in the abstract
and again here that "the majority of calculations are performed prior
to data taking during a training process." The fact that there is a
set-up phase that requires more calculations than are required to
analyze the data does not imply a reduction in computational cost, as
I feel the authors intend. It implies an enormous explosion in
computational cost: not only is there still the cost to analyze the
data, but now on top of that there is a huge start-up cost as well.
Again, I don't find a comparison of computational cost presented in
this manuscript. If the work presented here is a net reduction in
computational cost compared to normal matched filtering, then this
should be quantified somehow. By the way, as I mentioned above, modern
detection systems do not make use of matched filtering implemented in
the "convolve data with all waveforms at full sample rate" form,
however the ratio of the cost of doing that to the true cost of modern
systems is well documented elsewhere, so it's not necessary to compare
your system to real-world, as-implemented, filtering costs. It's
sufficient to compare the cost of your system to the full
matched-filtering approach, as that forms a common benchmark for
comparison.

An alternative is to simply not discuss the computational cost; remove
claims or suggestions of an improvement with respect to something
else. I think the technical achievement of demonstrating the
suitability of convolutional neural networks for GW signal detection
is sufficient to warrant publication. For example, some neural
networks can be efficiently implemented on FPGAs, which are not
necessarily suitable for traditional matched-filtering. Even if the
approach described here is more computationally costly than modern
matched-filtering on a traditional CPU, it's possible that there are
avenues available for hardware optimization that are not available to
matched-filtering. Therefore, how practical or not this approach is is
potentially a complex problem, and exploring that is likely beyond the
scope of this work.

If the authors have data on the relative performance of their system,
then by all means I encourage them to add that to this manuscript. I
merely mean to say that if they do not, then I would not require them
to spend the time obtaining it to proceed with publication. But in
that case they must remove claims regarding the performance of the
system.


# 5)* --- (Hunter) This statement has been removed. 

# (Hunter) We should show a reference where the computational cost
# of retraining a pre-trained network is minimal.

# (Hunter) Write in statement where we say "this is only a one-off
# cost. The cost of a template bank is repeated every single instance
# you apply it to the data.

# (Fergus) Perhaps we shouldn't mention the computational cost of
# training.

# (Michael) Once trained, the neural network can be used without
# any additional computational expense. 

# (Chris) Sn(f) is the signal sided detector noise PSD. This should
# be added to the text. Must define the FFT much better.

# (team response) It is true that training a neural network requires
# that some significant computational expense be spent. However,
# training is a one-off cost and does not need to be done for
# every new search analysis can be run in a matter of seconds. 
# (see ref Deep Learning Book). 

(team response) This comment is related to previous comments by the referee on
the alledged speed of the CNN approach. However, the specific statement in
question here is a true statement which we feel must be included in the
manuscript - the training stage constitutes the vast majority of the
computational cost. Since the statement is related to the contraversial issue
of speed we are happy to move the statement out of the abstract. We realise
that there is clearly some confusion over the issues of computational cost and
have therefore expanded the statement and placed it in the "simulation details"
section. It reads

"...used to quantify the performance of the trained network. In a practical
scenario the training and validation sets are used to train the network prior
to data taking. This constitutes the vast majority of computational effort and
is a procedure that needs to be computed only once. The trained network can
then be applied to test data at a vastly reduced cost in comparison to the
training stage (Ref Deep Learning book)."

Whilst we cannot currently compare the speed of the CNN method with that of
current matched filter analyses we can quote the approximate time spent in
training and applying the network to the data used in our simulation. We have
therefore added the following text to the final paragraph of the "Deep network
approach" section

"The computational time spent on training the network for each SNR is O(1) hour
on a single GPU. This one-time cost can be compared to the O(1) sec spent
applying the trained network to all 25,000 1 sec test data samples also using a
single GPU. Therefore at the point of data taking this particular analysis can
be run at ~10^4 times faster than real-time."


6. Page 2, second paragraph, "They typically then merge on the
timescale of O(1) sec ..." --> "They typically then merge on the
timescale of O(1 s) ...". Two violations of SI style conventions:
separating a number from its unit, and using a non-standard
abbreviation (https://physics.nist.gov/cuu/Units/rules.html).

# 6)* --- (Hunter) Apologies, I should have caught this. It's been fixed.

(team response) This SI style violation has been fixed. 


7. Equation (1) needs a reference. Otherwise you need define
normalization conventions for, e.g., Sn(f), and Fourier transforms.

# 7)* --- (Hunter) I forget now, but what paper did we pull this from? Babak et al.?

# (Hunter) Pulled from Babak et al. 

(Team response) A reference to Allen et al. (2012) has been added. We have also
clarified that the PSD is single-sided. It now reads

"...of the gravitational-wave strain and Sn(f) is the single-sided detector
noise PSD"

(Chris) Please change this referece to the Findchirp paper instead. We also
need to add that this is a single-sided PSD. So the text should say "and Sn(f)
is the single-sided detector noise PSD." 


8. Below equation (1), I'm puzzled by the extremely short interval of
data that has been used. 1 s is much less than the impulse length of
the filter required to whiten Advanced LIGO data (typically 10 s or
longer in length). Attempting to whiten so short a piece of data will
lead to undesirable, non-physical, boundary artifacts. This deserves
some explanation: how have they whitened it? Did they start with a
much larger piece, first, and extract the 1 s after? By what measure
is the result considered acceptable for the purposes of this analysis?
I realize there is a description of padding the data by 1/2 s on each
side, but that is still an order of magnitude too small under normal
circumstances. It's possible that because Gaussian noise is being used
the spectrum is sufficiently smooth, i.e., it lacks the narrow
spectral features of real data, for whitening so short an interval to
not be a problem. But, still, that should be explained because it
stands out as a surprising configuration choice.

# (Chris) Yes, it is indeed because Gaussian noise is being used.
# Paraphrase their words here in the paper.

# (Team response) Whitening is done using the detector noise power
# spectral density (PSD). Both the hplus and hcross waveforms are
# generated and stored in the frequency domain using LALSuite. A 
# lower frequency cutoff is chosen for each waveform such that the duration
# of the signal is 1 s and is dependent upon the chirp mass of the signal. 
# We divide the waveforms by the square root of 1/2 of the psd multiplied 
# by the sample rate in order to whiten the signal. Because we are 
# operating in Gaussian noise, the spectrum of the signal is smooth 
# enough to where there are no narrow spectral features which could 
# produce noticeable edge effects from the whitening process. 

8)* --- (Chris) The referee is correct, the Gaussianity of the noise and lack
of narrow spectral features has allowed us to use relatively short intervals of
data. The padding of 1/2 sec stretches of noise either side of the central
signal were all that was required to avoid the unphysical boundary effects.
Longer padding sizes were tested and we observed no change in whitening
behaviour for lengths >1/2 sec. We have added the following clarifying text in
the manuscript

"... Due to the requirements of the matched-filtering comparison it was
necessary to add padding to each time-series so as to avoid non-physical
boundary artefacts from the whitening procedure. The Gaussianity of the noise
and smoothness of the simulated advanced LIGO PSD allows the use of relatively
short padding. Therefore each 1 s timeseries has an additional 0.5 sec of
data prior to and after the signal."

9. Throughout this section, SI units are not being used correctly. "1
sec" --> "1 s", or "one second".
https://physics.nist.gov/cuu/Units/checklist.html

# 9)* --- (Hunter) Fixed.

(Team response) These SI unit violations have been fixed.


10. Equations (3) and (4) need references.

10)* --- (Hunter) Were these also taken from Babak et al.?

(Team response) References to Babak et al. have been added.


11. One of the things that I notice in Fig 2. is that the CNN does not
appear to offer any sensitivity improvement at higher or very low
SNRs, but it appears to offer one in the marginal SNR regime, e.g.,
SNR=4. By chance this corresponds to the SNR at which matched-filter +
trigger systems give up. For example the GstLAL detection system
operates at a trigger threshold of 4. Because of the very high
false-alarm rate, and the data management burden that creates, it is
not practical to collect triggers at a lower SNR threshold. It is
known, however, that doing so would improve the sensitivity of the
search. If a system was a available for producing triggers in the
range, say, 3 <= SNR <= 4 and this system had a lower false-alarm rate
than matched filtering in that regime, then that would allow detection
systems to operate at a lower threshold. It's possible some sort of
hybrid system would be best, where a CNN is responsible for
identifying the very low SNR triggers and a traditional
matched-filtering system is used for higher SNR things. I don't have
any specific comment or request to make in this regard, only that if
the issue of the performance at higher and lower SNRs is raised, then
pointing out the utility in the marginal SNR regime, would be apropos.

# 11)* --- (Hunter) I tend to agree with their assesment that this would be
# best as some sort of hybrid system. Perhaps in more practical terms it 
# would be most useful as a complimentary detection statistic to chi^2?

# (Chris) Think bigger. Could use this opportunity to say we can replace
# everything with a CNN.

# (Team response) 

11)* --- (Chris) We agree with the referee that this is an interesting result
that deserves further study and that the idea of a hybrid matched-filter-CNN
approach would be possible. However, we prefer not to make a statement on the
possibility of hybridising the approaches for two reasons. Firstly we would
note that this is our first real attempt at CNN application and there is a lot
of flexibility in the approach and much development still to do. We would not
rule out the strong possibility that further developed CNN (or other machine
learning) analyses could outperform or exactly match matched-filtering at all
SNR regimes. Secondly, despite a current lack of evidence, we feel that one of
the key features of this analysis is its speed and its potential as a rapid
parameter estimation tool independent of matched filtering.  

We thank referee A for their very helpful comments.

##################
Referee B Comments
##################


1. Does the method scale up to searches over the full gravitational
waveform parameter space, including the parts that would be covered
(in a matched-filter search) by very closely spaced and very long
waveforms? (The test case used this part of parameter space that is in
the opposite extreme, namely short and well spaced signals.)

1)* --- (Hunter) The CNN aproach could in principle also be applied to other
CBC searches such as BNS, NSBH, and LISA sources. Regarding the increased
duration of such waveforms, the main limitation here is related to the amount of
memory required to train such a network. We currently train on a Tesla P-100
GPU which has ~16GB of memory and use most of this when training on only
500,000 1sec long samples. BNS signals are ~100 times longer and LISA signals
longer still. This is a technical issue that may require improvements in both
hardware and software.

Regarding the potential close spacing and overlap of signals we propose simply
running the CNN analysis on overlapping time segments. In this sense the
analysis is similar to traditional matched-filtering approaches where the
template is convolved with the data and therefore maximised over the signal
arrival time. The issue of multiple overlapping signals is relevant for 3rd
generation and space-based detectors and is not resolved even for
matched-filtering searches. We leave this important study for future work.   


2. Can the authors make any kind of quantitative statement about the
comparison in computational costs between the neural network method
and the matched filter technique? And if so, does that scale in a
benign way as the neural network is expanded to the full parameter
space?

# 2)* --- (Hunter) Perhaps it would be best to remove claims made about 
# computational cost. If this has been done, do we still have to 
# respond to this comment?

2)* --- (Chris) The issue of speed comparison has been the main focus of
referee A. Based on referee A's commments we have decided to remove all vague
claims of speed improvement over matched-filtering approaches. We feel that
this is a very important question to answer but at present we are not able to
provide quantitative evidence for relative speed improvements. However, we have
now added text (see response to Ref A comment 5) stating the approximate
computational running times for the CNN analysis in this BBH-specific case.  

We thank referee B for their very helpful comments.

#################
Reinhard Comments
#################

1.) I'm a bit surprised you're not citing the 2nd George&Huerta paper
(using real aLIGO data) https://arxiv.org/abs/1711.03121, and also that
there isn't more discussion about the relation between your results and
https://arxiv.org/abs/1701.00008, ie what's better/worse/different in
your approach etc ...

1* --- (Chris) In the rush and general confusion when the 3 separate teams submitted their papers we simply forgot to add G&H’s second paper. We really should add it. It will look strange since we do very similar things. However, that’s just how it is. Regarding a comparison to their first paper. This would have been embarrassing to G&H since their 1st paper results are incorrect and very poorly communicated. We made the conscious decision to avoid saying anything since everything we attempted writing was likely to provoke a negative response from G&H.

(Hunter) I ended up electing to cite their most recent publication in
Physics Letters B.

Also add Gebhard et al

@conference{GebKilParHarSch,
  title = {ConvWave: Searching for Gravitational Waves with Fully Convolutional Neural Nets},
  author = {Gebhard, T. and Kilbertus, N. and Parascandolo, G. and Harry, I. and Sch{\"o}lkopf, B.},
  booktitle = {Workshop on Deep Learning for Physical Sciences (DLPS) at the 31st Conference on Neural Information Processing Systems (NIPS)},
  year = {2017},
  url = {https://dl4physicalsciences.github.io/files/nips_dlps_2017_13.pdf}
}

2.) I was a bit confused about the numbers when you talk about the
training set of 4e5 timeseries, 50% contain signal+noise ie 2e5, but
then you say there's 1000 unique signals, and for each you use 25 noise
realizations, but that's 2.5e4 signal timeseries, so how does that make
2e5. I probably mis-understood something.

2* --- (Chris) That is a historical typo and was true at some point. The 1000 number should be changed.

(Hunter/Michael) 5e5 timeseries and 10,000 unqiue signals. Still need 
to add this into the paper.

(Hunter) This has been fixed.


3.) Your loss function Eq.(2): I'm not quite understanding that equation
(without going back to remind myself from the literature): on the lhs
there is f(theta), but then on the rhs there's theta_i^{S/N} so I'm
confused about the notation: is 'theta' a vector consisting of
theta_i^{S/N}?
Also, shouldn't there be some relationship between 'correct answer' and
'actual answer' in there, and right now the text seems to suggest that
the theta_i^{S/N} are the outputs from the network, ie the 'actual
answer', but how does the 'loss vs correct answer' come in here?

3* --- (Chris) You’re right that this is a confusing equation. Theta is a set of estimated probabilities. Typically this is done in batches but just imagine a set of output probabilities of which ~half were for signals and half were for just noise realisations. You would hope that for all of the signals the probability is high and for all the noises it’s low. The bad notation here is that theta_S is the probability that the data contains a signal and theta_N is the probability that the data is just noise. So for a 2-class system we have 1 = theta_S + theta_N. The loss function is just the sum of the signal log-probs *for the signals* plus the sum of the noise log-probs *for the noises*. This is minimised when all theta_S = 1 and all theta_N = 1. Make sense?

4.) if I were the referee of this paper, I would be slightly concerned
about the offhand claim that "This latter discrepancy can be mitigated
by increasing the number of training samples", when discussing Fig.3,
which sounds like a strong claim without any actual evidence?

4* --- (Chris) Me too. This was a last minute addition to try to describe the efforts we made in the last week of the analysis. We found that we needed to really push on the training data to get the ML efficiencies to compete with matched filtering in the low false alarm regime. Time constraints and word limits are the main reason. I hope to be able to expand upon this based on referees comments.

(Hunter/Michael) Perhaps a better statement would be "could be mitigated" rather than "can". Something more ambiguous.

(Hunter) Michael's suggestion has been implemented.


5.) And also I might wonder if there could be more discussion/justification
for why it is possible/plausible that the network beats
matched-filtering, which is often believed to be close to optimal. I can
believe your results, but I just wonder if one could comment more on
*why* the current matched-filtering seems to have room for improving on
it. Gaussian noise is mentioned, but your results *are* on Gaussian
noise, so it can't be that, right? Is it the template bank? But 3%
mismatch seems quite low, would that be sufficient to explain the
improvement?.

5* --- (Chris) We would also like to do this. However, we honestly don’t know the definitive answer yet. The CBC group and Tom Dent seem to think that this is due to the difference in density of templates vs the prior in the mass space. Higher numbers of *independent* templates give higher false alarms. There might be some optimal balance (e.g. Christian Roever’s paper) for template placement that resolves this. Further work is required. Again, the main constraint is the word count.

