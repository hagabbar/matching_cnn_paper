Dear Dr. Abhishek Agarwal,

Thank you for sending us the referees reports, who we thank for
reading our manuscript and for providing very useful input. We are delighted by
the overall positivity of the referees and have addressed their
comments in the text below.

Best wishes,

H. Gabbard, M. Williams, F. Hayes, and C. Messenger

##################
Referee A Comments
##################

1. It is claimed in a number of places in the text that the
computational cost of searches for compact object mergers is tied to
improvements in the low-frequency sensitivity of gravitational-wave
detectors. For example, in the abstract there is the statement

"However, the computational cost of such searches in low latency will
grow dramatically as the low frequency sensitivity of
gravitational-wave detectors improves."

and in the second paragraph of the Introduction

"The computational cost to run the search analysis is due to the large
parameter space and the increasing cost of analyzing longer duration
waveforms as the low frequency sensitivity of the detectors improves."

These claims are not true. They would be true for a naive,
brute-force, template bank + matched-filter approach, as was used in
the early days of LIGO, but they are not true for modern detection
systems. The low frequency parts of the signals have two properties
relevant here: firstly, they're low frequency meaning the filtering of
these parts of the waveforms can be computed at a lower sample rate;
secondly the rate of frequency evolution at low frequencies is small
due to the slow rate of energy loss from the system, making the
structure of the waveforms more and more simple at low frequencies.
Modern detection systems, for example the Multi-Band Template Analysis
(MBTA) and GstLAL-based detection systems, take advantage of both of
these properties to remove redundancy from the calculations and reduce
the computational cost. The result is that the cost is nearly
independent of the starting frequency; the computational cost is
entirely dominated by the high frequency parts of the waveform where
high data sample rates are required and complex frequency and
amplitude evolution are observed.

I cannot find anywhere in this document an accounting of the
computational cost of the convolutional neural network nor of the
matched filtering approach, and I feel that without that information a
discussion of the computational cost of either of the approaches, or
in particular their scalings, is out of place. I will raise this issue
again below, but I think perhaps removing any sort of claims of speed,
cost, whatever, is warranted, unless a comparative study is presented
to support them.

(team response) We believe that the referee is entirely correct in asking for
these claims to be removed from the paper. We have therefore removed statements
that make explicit claims of speed improvements over current advanced matched
filtering approaches. We have now included a statement in the manuscript text
that gives the actual time taken to both train and run the CNN analysis but we
do not compare these values with speeds or scaling relations for existing
matched-filter approaches (see our response to comment 5 for the additional
text). As the referee points out in further comments below, the main result of
the paper alone is worthy of publication in PRL. We will leave a detailed study
of comparative computational cost to future work.  

2. Introduction, second paragraph, "... in these search pipelines is
achieved using a technique known as ..." --> ... in these search
pipelines is achieved, in part, using a technique known as ..." In
fact the process also makes of use various \chi^2-like statistics, and
various measures of signal consistency across the multiple detectors
world-wide.

(team response) This sentence has been changed accordingly.

3. Introduction, third paragraph, "... each with different component
mass components and/or ..." --> "... each with different component
mass and/or ..."

(team response) This typo has been fixed.


4. Introduction, fourth paragraph, the claim "This results in
low-latency searches that can be orders of magnitude faster than other
comparable classification methods." requires a reference. There are
weasel words in there, "can be", "comparable" (what are "comparable"
methods? only the ones for which this statement is true?), and because
of that strictly speaking no specific claim is being made here, but a
claim is being implied and I am certain that claim is false. I am
reminded of similar claims made about GPU-based FFT algorithms: orders
of magnitude faster, 100x faster, and so on. It is true that the GPU
can perform a subset of the tasks required to identify
gravitational-wave signals at an enormously higher speed than a CPU,
but it can't do all of the tasks at that speed. If 50% of the tasks
can be done so quickly that they are reduced to 0 time, you have only
increased the speed by a factor of 2, not 100s or 1000s, because there
remains the other 50% of the tasks that still take just as long.
Meanwhile, if you merely wait 18 months Moore's Law will give you that
factor of 2, anyway, without having to spend time porting your
algorithm to GPUs. This is the Achilles' heal of GPUs: they're a lot
of work, and there are things they're not good at, which is why the
use of GPUs for matched filtering based GW detection systems has never
gained traction. Anyway, if a reference to support this claim cannot
be found it should be removed.

(team response) This point is related to the first comment and we are happy to
remove statements that refer to definitive speed improvements over traditional
methods. We do this for the clear scientific reason given by the referee, that
we have no concrete evidence of this at the present time.

However, we would like to clarify some key points to the referee in light of
their statement that they are certain our speed claims are false. This sounds
equally like a statement without evidence to support it. Historical claims
regarding the speed improvement using GPUs are not relevant here. Yes, GPUs are
key to the sucess of neural network applications in recent years but the more
important (potential) speed gains are due to the algorithm itself together with
the division of computational effort into training and testing. It is possible that
the "total" computational cost of CNNs exceeds that of traditional techniques,
but only in the training stage prior to the analysis. The testing stage is
incredibly fast but as highlighted by the referee, we do not have any scaling
relations to refer to for CNNs at present. For the referees information, the
analysis of 25K 1sec segments of data runs on a single GPU in ~few sec.

5. I am also puzzled by the claim that has been made in the abstract
and again here that "the majority of calculations are performed prior
to data taking during a training process." The fact that there is a
set-up phase that requires more calculations than are required to
analyze the data does not imply a reduction in computational cost, as
I feel the authors intend. It implies an enormous explosion in
computational cost: not only is there still the cost to analyze the
data, but now on top of that there is a huge start-up cost as well.
Again, I don't find a comparison of computational cost presented in
this manuscript. If the work presented here is a net reduction in
computational cost compared to normal matched filtering, then this
should be quantified somehow. By the way, as I mentioned above, modern
detection systems do not make use of matched filtering implemented in
the "convolve data with all waveforms at full sample rate" form,
however the ratio of the cost of doing that to the true cost of modern
systems is well documented elsewhere, so it's not necessary to compare
your system to real-world, as-implemented, filtering costs. It's
sufficient to compare the cost of your system to the full
matched-filtering approach, as that forms a common benchmark for
comparison.

An alternative is to simply not discuss the computational cost; remove
claims or suggestions of an improvement with respect to something
else. I think the technical achievement of demonstrating the
suitability of convolutional neural networks for GW signal detection
is sufficient to warrant publication. For example, some neural
networks can be efficiently implemented on FPGAs, which are not
necessarily suitable for traditional matched-filtering. Even if the
approach described here is more computationally costly than modern
matched-filtering on a traditional CPU, it's possible that there are
avenues available for hardware optimization that are not available to
matched-filtering. Therefore, how practical or not this approach is is
potentially a complex problem, and exploring that is likely beyond the
scope of this work.

If the authors have data on the relative performance of their system,
then by all means I encourage them to add that to this manuscript. I
merely mean to say that if they do not, then I would not require them
to spend the time obtaining it to proceed with publication. But in
that case they must remove claims regarding the performance of the
system. 

(team response) This comment is related to previous comments by the referee on
the alledged speed of the CNN approach. However, the specific statement in
question here is a true statement which we feel must be included in the
manuscript - the training stage constitutes the vast majority of the
computational cost. Since the statement is related to the contraversial issue
of speed we are happy to move the statement out of the abstract. We realise
that there is clearly some confusion over the issues of computational cost and
have therefore expanded the statement and placed it in the "simulation details"
section. It reads

"...used to quantify the performance of the trained network. In a practical
scenario the training and validation sets are used to train the network prior
to data taking. This constitutes the vast majority of computational effort and
is a procedure that needs to be computed only once. The trained network can
then be applied to test data at a vastly reduced cost in comparison to the
training stage (Ref Deep Learning book)."

Whilst we cannot currently compare the speed of the CNN method with that of
current matched filter analyses we can quote the approximate time spent in
training and applying the network to the data used in our simulation. We have
therefore added the following text to the final paragraph of the "Deep network
approach" section

"The computational time spent on training the network for each SNR is O(1) hour
on a single GPU. This one-time cost can be compared to the O(1) sec spent
applying the trained network to all 25,000 1 s test data samples also using a
single GPU. Therefore at the point of data taking this particular analysis can
be run at ~10^4 times faster than real-time."


6. Page 2, second paragraph, "They typically then merge on the
timescale of O(1) sec ..." --> "They typically then merge on the
timescale of O(1 s) ...". Two violations of SI style conventions:
separating a number from its unit, and using a non-standard
abbreviation (https://physics.nist.gov/cuu/Units/rules.html).

(team response) This SI style violation has been fixed. 


7. Equation (1) needs a reference. Otherwise you need define
normalization conventions for, e.g., Sn(f), and Fourier transforms.

(Team response) A reference to Allen et al. (2012) has been added. We have also
clarified that the PSD is single-sided. It now reads

"...of the gravitational-wave strain and Sn(f) is the single-sided detector
noise PSD"

(Chris) Please change this referece to the Findchirp paper instead. We also
need to add that this is a single-sided PSD. So the text should say "and Sn(f)
is the single-sided detector noise PSD." 


8. Below equation (1), I'm puzzled by the extremely short interval of
data that has been used. 1 s is much less than the impulse length of
the filter required to whiten Advanced LIGO data (typically 10 s or
longer in length). Attempting to whiten so short a piece of data will
lead to undesirable, non-physical, boundary artifacts. This deserves
some explanation: how have they whitened it? Did they start with a
much larger piece, first, and extract the 1 s after? By what measure
is the result considered acceptable for the purposes of this analysis?
I realize there is a description of padding the data by 1/2 s on each
side, but that is still an order of magnitude too small under normal
circumstances. It's possible that because Gaussian noise is being used
the spectrum is sufficiently smooth, i.e., it lacks the narrow
spectral features of real data, for whitening so short an interval to
not be a problem. But, still, that should be explained because it
stands out as a surprising configuration choice. 

(team response) The referee is correct, the Gaussianity of the noise and lack
of narrow spectral features has allowed us to use relatively short intervals of
data. The padding of 1/2 sec stretches of noise either side of the central
signal were all that was required to avoid the unphysical boundary effects.
Longer padding sizes were tested and we observed no change in whitening
behaviour for lengths >1/2 sec. We have added the following clarifying text in
the manuscript

"... Due to the requirements of the matched-filtering comparison it was
necessary to add padding to each time-series so as to avoid non-physical
boundary artefacts from the whitening procedure. The Gaussianity of the noise
and smoothness of the simulated advanced LIGO PSD allows the use of relatively
short padding. Therefore each 1 s timeseries has an additional 0.5 s of
data prior to and after the signal."

9. Throughout this section, SI units are not being used correctly. "1
sec" --> "1 s", or "one second".
https://physics.nist.gov/cuu/Units/checklist.html

(Team response) These SI unit violations have been fixed.


10. Equations (3) and (4) need references.

(Team response) References to Babak et al. have been added.


11. One of the things that I notice in Fig 2. is that the CNN does not
appear to offer any sensitivity improvement at higher or very low
SNRs, but it appears to offer one in the marginal SNR regime, e.g.,
SNR=4. By chance this corresponds to the SNR at which matched-filter +
trigger systems give up. For example the GstLAL detection system
operates at a trigger threshold of 4. Because of the very high
false-alarm rate, and the data management burden that creates, it is
not practical to collect triggers at a lower SNR threshold. It is
known, however, that doing so would improve the sensitivity of the
search. If a system was a available for producing triggers in the
range, say, 3 <= SNR <= 4 and this system had a lower false-alarm rate
than matched filtering in that regime, then that would allow detection
systems to operate at a lower threshold. It's possible some sort of
hybrid system would be best, where a CNN is responsible for
identifying the very low SNR triggers and a traditional
matched-filtering system is used for higher SNR things. I don't have
any specific comment or request to make in this regard, only that if
the issue of the performance at higher and lower SNRs is raised, then
pointing out the utility in the marginal SNR regime, would be apropos.

(team response) We agree with the referee that this is an interesting result
that deserves further study and that the idea of a hybrid matched-filter-CNN
approach would be possible. However, we prefer not to make a statement on the
possibility of hybridising the approaches for two reasons. Firstly we would
note that this is our first real attempt at CNN application and there is a lot
of flexibility in the approach and much development still to do. We would not
rule out the strong possibility that further developed CNN (or other machine
learning) analyses could outperform or exactly match matched-filtering at all
SNR regimes. Secondly, despite a current lack of evidence, we feel that one of
the key features of this analysis is its speed and its potential as a rapid
parameter estimation tool independent of matched filtering.  

We thank referee A for their very helpful comments.

##################
Referee B Comments
##################


1. Does the method scale up to searches over the full gravitational
waveform parameter space, including the parts that would be covered
(in a matched-filter search) by very closely spaced and very long
waveforms? (The test case used this part of parameter space that is in
the opposite extreme, namely short and well spaced signals.)

(team response) The CNN aproach could in principle also be applied to other
CBC searches such as BNS, NSBH, and LISA sources. Regarding the increased
duration of such waveforms, the main limitation here is related to the amount of
memory required to train such a network. We currently train on a Tesla P-100
GPU which has ~16GB of memory and use most of this when training on only
500,000 1sec long samples. BNS signals are ~100 times longer and LISA signals
longer still. This is a technical issue that may require improvements in both
hardware and software.

Regarding the potential close spacing and overlap of signals we propose simply
running the CNN analysis on overlapping time segments. In this sense the
analysis is similar to traditional matched-filtering approaches where the
template is convolved with the data and therefore maximised over the signal
arrival time. The issue of multiple overlapping signals is relevant for 3rd
generation and space-based detectors and is not resolved even for
matched-filtering searches. We leave this important study for future work.   


2. Can the authors make any kind of quantitative statement about the
comparison in computational costs between the neural network method
and the matched filter technique? And if so, does that scale in a
benign way as the neural network is expanded to the full parameter
space?

(team response) The issue of speed comparison has been the main focus of
referee A. Based on referee A's commments we have decided to remove all vague
claims of speed improvement over matched-filtering approaches. We feel that
this is a very important question to answer but at present we are not able to
provide quantitative evidence for relative speed improvements. However, we have
now added text (see response to Ref A comment 5) stating the approximate
computational running times for the CNN analysis in this BBH-specific case.  

We thank referee B for their very helpful comments.





