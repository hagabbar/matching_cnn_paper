\relax 
\citation{PhysRevLett.116.061102,PhysRevLett.116.241103,PhysRevLett.118.221101}
\citation{PhysRevLett.119.141101}
\citation{0264-9381-33-21-215004}
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {title}{Matching Matched Filtering with Deep Networks in Gravitational wave Astronomy}{1}{}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{}}
\newlabel{eq:loss}{{1}{1}{}{}{}}
\newlabel{eq:snr}{{2}{2}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Description here.}}{2}{}}
\newlabel{fig:waveform}{{1}{2}{}{}{}}
\newlabel{eq:nesterov1}{{3}{2}{}{}{}}
\newlabel{eq:nesterov2}{{4}{2}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  The loss, accuracy and learning rate plots (shown above) illustrate how the network's performance is defined as a function of the number of training epochs. The goal is to minimize the loss function, which will in turn maximize the accuracy of the classifier. The first initial epochs see an exponential decrease in the loss function and then a slowly falling monotonic curve to follow. This indicates that the longer our network is trained, a limit with respect to the accuracy is approached. In our case, we cyclically adjust the learning rate to oscialte between $5 \times 10^{-4}$ and $1 \times 10^{-3}$ at a constant frequency. Studies have shown that this policy of learning rate adjustement (\textbf  {should replace figure with better run}) }}{3}{}}
\newlabel{fig:loss_curve}{{2}{3}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Confusion matrices for runs from iSNR 2 - iSNR 12. The accuracies for all are listed as follows: $50.19\%$ at iSNR 2, $68.34\%$ at iSNR 4, $88.72\%$ at iSNR 6, $97.88\%$ at iSNR 8, $99.64\%$ at iSNR 10 and $99.86\%$ at iSNR 12. Note that runs with iSNR 2 give results that are equivalent randomized guessing.}}{3}{}}
\newlabel{fig:confusion}{{3}{3}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \textbf  {Place description here.}}}{3}{}}
\newlabel{fig:ROC_curve}{{4}{3}{}{}{}}
\bibdata{match_cnn_paperNotes,references}
\bibcite{PhysRevLett.116.061102}{{1}{2016{}}{{Abbott\ \emph  {et~al.}}}{{Abbott \emph  {et~al.}}}}
\bibcite{PhysRevLett.116.241103}{{2}{2016{}}{{Abbott\ \emph  {et~al.}}}{{Abbott \emph  {et~al.}}}}
\bibcite{PhysRevLett.118.221101}{{3}{2017{}}{{Abbott\ \emph  {et~al.}}}{{Abbott \emph  {et~al.}}}}
\bibcite{PhysRevLett.119.141101}{{4}{2017{}}{{Abbott\ \emph  {et~al.}}}{{Abbott \emph  {et~al.}}}}
\bibcite{0264-9381-33-21-215004}{{5}{2016}{{Usman\ \emph  {et~al.}}}{{Usman, Nitz, Harry, Biwer, Brown, Cabero, Capano, Canton, Dent, Fairhurst, Kehl, Keppel, Krishnan, Lenon, Lundgren, Nielsen, Pekowsky, Pfeiffer, Saulson, West,\ and\ Willis}}}
\bibstyle{apsrev4-1}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The optimal network structure (seen below) was determined through multiple tests and tunnings of hyperparameters by means of trial and error. The network consists of 8 convolutional layers, followed by 2 fully-connected layers. Max-pooling is performed on the first, fifth, and eigth layer, whereas dropout is only performed on the two fully-connected layers. Each layer uses an Elu activation function while the last layer uses a Softmax activation function in order to normalize the output values to be between zero and one so as to give a probability value for each class. \\}}{4}{}}
\newlabel{table:network}{{I}{4}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \textbf  {Place description here.}}}{4}{}}
\newlabel{fig:isnr_curves}{{5}{4}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{4}{}}
\newlabel{LastBibItem}{{5}{4}{}{}{}}
\newlabel{LastPage}{{}{4}{}{}{}}
